{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZftuWFSZIk9XKRl1vjNIo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarenales/Webstraping-Booking/blob/main/WebstrappingBooking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Trabajo optativo de la asignatura de Minería de Datos hecho por Silvia Arenales.**\n",
        "\n",
        "Para este trabajo se nos pide realizar WebStraping de páginas web o usando APIs, para recoger datos y así crear un problema de clasificación. Esto es recoger datos, muchos datos, para luego crear una matríz con variables predictoras y la variable clase a predecir.\n",
        "\n",
        "Primeramente, tenemos que buscar dónde vamos a extraer la información. Hay muchas páginas web que se puede utilizar para hacer webstraping, dependiendo de lo que se quiera buscar.\n",
        "\n",
        "- **Wikipedia**: es una gran fuente de información y su contenido es generalmente de dominio público, lo que significa que está disponible para su uso y reutilización.\n",
        "\n",
        "- **Páginas web de noticias**: Muchos sitios web de noticias publican cientos de artículos cada día. Puedes utilizar web scraping para extraer títulos y contenidos de estos artículos y luego clasificarlos por tema o categoría.\n",
        "\n",
        "- **Redes sociales**: Las redes sociales como Twitter y Facebook contienen una gran cantidad de datos que podrías utilizar para hacer web scraping y clasificación. Por ejemplo, podrías extraer tweets o publicaciones de Facebook y clasificarlos en categorías como noticias, deportes, entretenimiento, etc.\n",
        "\n",
        "- **Páginas web de comercio electrónico**: Si quieres hacer web scraping de productos de una página web de comercio electrónico, puedes utilizar web scraping para extraer información sobre precios, descripciones, imágenes, etc. y luego clasificar los productos por categoría o marca. (Amazon por ejemplo).\n",
        "\n",
        "También hay que mencionar que hay que tener en cuenta de que algunas de estas páginas web puede tener término de uso que prohíben el webscraping o requieren un permiso especial para hacerlo.\n",
        "\n",
        "Yo para este trabajo me había decantado por hacer Webstraping por medio de **Twitter** por medio del lenguaje de programación Python, pero, tras unos días de espera tras haberles escrito varias veces, no optube las auterizaciones de twiter-developer, entonces tuve que optar por otra idea más básica para poder hacerlo.\n",
        "\n",
        "Se me ocurrió hacer webstraping en alguna página de viajes como booking para determinar el precio de una habitación en función de sus características. \n",
        "\n",
        "Por tanto, el **problema de clasificación** es el siguiente: \n",
        "En función de unas caracterísiticas ( variables predictoras como la localización, numero de comentarios, desayuno incluido, cancelación ...) predecir el precio de una habitación en Nueva York, en los días 20 de enero de 2023 al 22 de enero de 2023.\n",
        "\n",
        "Para realizar estacción de datos he utilizado la librería BeautifulSoup de Python.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ui_pMJbCDbB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install BeautifulSoup"
      ],
      "metadata": {
        "id": "YyFIRderNgPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "También he utilizado las siguientes librerías:\n",
        "\n",
        "Requests para realizar solicitudes a la página web de donde estamos extrayendo los datos."
      ],
      "metadata": {
        "id": "4U-1o38cNi8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install requests"
      ],
      "metadata": {
        "id": "_5Kf6WkGNidC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Csv para poder visualizar nuestra extracción en un documento excel."
      ],
      "metadata": {
        "id": "6i1buh1wN_4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install csv"
      ],
      "metadata": {
        "id": "SZMK0AYNNrhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explicación del algoritmo\n",
        "\n",
        "Primero importamos las librerías que utilizaremos"
      ],
      "metadata": {
        "id": "_d2ji0GxU8dH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import csv"
      ],
      "metadata": {
        "id": "XcwjMH19VKse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Guardamos en una variable la url de donde extraemos los datos."
      ],
      "metadata": {
        "id": "Hcil_O_CVOj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "URL1 = 'https://www.booking.com/searchresults.es.html?ss=Nueva+York%2C+New+York+State%2C+Estados+Unidos&efdco=1&label=es-JCB2UqznXtCO_RDP_nj5CAS410545262609%3Apl%3Ata%3Ap1%3Ap22.563.000%3Aac%3Aap%3Aneg%3Afi%3Atikwd-65526620%3Alp1005530%3Ali%3Adec%3Adm%3Appccp%3DUmFuZG9tSVYkc2RlIyh9Ye8F2ouj63ytkBtrYs5TAfs&aid=376371&lang=es&sb=1&src_elem=sb&src=index&dest_id=20088325&dest_type=city&ac_position=0&ac_click_type=b&ac_langcode=es&ac_suggestion_list_length=5&search_selected=true&search_pageview_id=0f1e5f85fa8200e4&ac_meta=GhAwZjFlNWY4NWZhODIwMGU0IAAoATICZXM6BE5ldyBAAEoAUAA%3D&checkin=2023-01-20&checkout=2023-01-22&group_adults=2&no_rooms=1&group_children=0&sb_travel_purpose=leisure'"
      ],
      "metadata": {
        "id": "G515VZ0zVMor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para evistar que la página web nos bloquee la petición, le pasamos un User-Agent"
      ],
      "metadata": {
        "id": "i4eXkqDbVdy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "headers={\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36\"}\n"
      ],
      "metadata": {
        "id": "Gri9upbfVnLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora realizamos la petición a la página web. Si todo está OK su respuesta será de 200."
      ],
      "metadata": {
        "id": "js312pmCVpK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = requests.get(URL1, headers=headers)\n",
        "print(response.status_code)"
      ],
      "metadata": {
        "id": "wi2qcRKyV5IN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "O4f_twszV4u7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cont = 0\n",
        "soup = BeautifulSoup(response.text, 'lxml')\n",
        "if response.status_code == 200: \n",
        "    # abrir un fichero csv para guardar los datos\n",
        "    with open('hoteles.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        # crear un objeto DictWriter para escribir los datos en el fichero csv\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=['nombre', 'url', 'localizacion','numero coments','desayuno','cancelacion','precio'])\n",
        "        # escribir los encabezados\n",
        "        writer.writeheader()\n",
        "        for item in soup.select('.a826ba81c4.fe821aea6c.fa2f36ad22.afd256fc79.d08f526e0d.ed11e24d01.ef9845d4b3.da89aeb942'):\n",
        "            try:\n",
        "                print(cont)\n",
        "                print('---------------------')\n",
        "                name = item.select('.fcab3ed991.a23c043802')[0].get_text().strip()\n",
        "                print(name)  # name\n",
        "                url = item.select('.e13098a59f')[0]['href']\n",
        "                print(url)   # url\n",
        "                location = item.select('.f4bd0794db.b4273d69aa')[0].get_text().strip()\n",
        "                print(location)  # location\n",
        "                num_coments = item.select('.d8eab2cf7f.c90c0a70d3.db63693c62')[0].get_text().strip().split()[0]\n",
        "                print(num_coments) #hw many commets\n",
        "                try:\n",
        "                    b = item.select('.e05969d63d')[0].get_text().strip()\n",
        "                    breakfast = 1\n",
        "                    print(breakfast) # breakfast?\n",
        "                except Exception as e2:\n",
        "                    breakfast = 0\n",
        "                    print(breakfast)\n",
        "                try:\n",
        "                    c = item.select('.d506630cf3')[0].get_text().strip()\n",
        "                    cancelation = 1\n",
        "                    print(cancelation)    # cancelation\n",
        "                except Exception as e1:\n",
        "                    cancelation = 0\n",
        "                    print(cancelation)\n",
        "                # clase a predecir\n",
        "                price = item.select('.fcab3ed991.fbd1d3018c.e729ed5ab6')[0].get_text().strip().split('$')[1]\n",
        "                print(price)  # price\n",
        "\n",
        "                # escribir los datos en el fichero csv\n",
        "                writer.writerow({\n",
        "                  'nombre':name, \n",
        "                    'url':url, \n",
        "                    'localizacion':location,\n",
        "                    'numero coments':num_coments,\n",
        "                    'desayuno':breakfast,\n",
        "                    'cancelacion':cancelation,\n",
        "                    'precio': price\n",
        "                })\n",
        "                cont +=1 \n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "else:\n",
        "    print('Status Failed On', response.status_code)"
      ],
      "metadata": {
        "id": "EjgGsPQkLKnr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}